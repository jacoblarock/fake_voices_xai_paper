\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{biblatex} % Imports biblatex package
\addbibresource{references.bib}
\usepackage{changepage}
\usepackage{parskip}

\title{Creating More Meaningful Explanations for Audio Deepfake Detection}
\date{2024-10-01} 
\author{Jacob LaRock\\1667321}
\linespread{1.5}

\def\Institute{Universität Siegen}
\def\KindOfWork{Bachelorarbeit}
\def\Studiengang{Wirtschaftsinformatik}
\def\Fakultaet{Fakultät III: Wirtschaftswissenschaften, Wirtschaftsinformatik und Wirtschaftsrecht}
 
\def\Title{Creating More Meaningful Explanations for Audio Deepfake Detection}
\def\Subtitle{}
 
\def\student{Jaocb LaRock}
\def\studentno{1667321}
\def\place{Siegen}
\def\Date{31.03.2025} 
\def\semester{6}
 
\def\erstpruefer{Gunnar Stevens}
\def\erstprueferMail{Gunnar.Stevens@uni-siegen.de}

\begin{document}
	\begin{titlepage}
        \begin{minipage}{0.9\linewidth}
			\centering
			\includegraphics [width=0.35\textwidth]{images/LogoSiegen}
        \end{minipage}

		\vspace{2cm}

		\centering

		{\Large\bfseries \Title\par}
		{\large\bfseries \Subtitle\par}
		\vspace{2cm}

		{\large \textbf{\KindOfWork}\par}

		\vspace{1.5cm}

		{\normalsize im Studiengang \\
			\Studiengang  im \semester. Semester \\
		}
		\vspace{0.5cm}
		{\normalsize vorgelegt von}
		\vspace{0.5cm}

		{\normalsize \textbf{\student} \\
			Matr.-Nr.: \studentno  \\}
		{am \normalsize \Date\par}

		\vspace{0.5cm}

		{\Institute} \par
		{\Fakultaet}\par

		\vspace{1cm}

    {\begin{table}[h]
			\centering
			\begin{tabular}{lll}
				{Erstprüfer:} & \erstpruefer & \erstprueferMail \\
			\end{tabular}
		\end{table}}     
		\vfill
	\end{titlepage}
    \newpage
    \section{Abstract}
    Abstract filler text blablabla!
    \section{Introduction}
    As the use of generative methods for the creation of synthetic voices becomes more widespread,
    so grows the need for reliable and usable detection methods for the protection of the security
    of people and businesses alike. In particular, the rise of deepfake technology has led to
	concerns about its potential misuse in areas such as politics, entertainment, and national
	security by, for instance, potentially allowing malicious actors to create fake audio
	recordings that appear to be genuine statements made by public figures or to create
	genuine-seeming recordings of events that never happened, which could have significant
	consequences if used to spread misinformation, from defaming individuals to creating political
	tension \cite{veerasamy_rising_2022,albahar_deepfakes_2005}. The field of Explainable AI (XAI)
	shows promise for producing useful and interpretable results from such models. Explainable AI
	refers to the ability of artificial intelligence systems, such as machine learning models and
	neural networks, to provide understandable and interpretable explanations for their decisions
	or predictions \cite{hind_explaining_2019}. This means that XAI systems can articulate why
	they made a particular decision, what factors influenced it, and how confident they are in
	their conclusion \cite{hind_explaining_2019}. For the detection of audio deepfakes, this could
	mean that the model can justify its classification with feature-based evidence, allowing for
	verification of the result by the end user. Much of the existing research, further discussed
	in the next section, follows the path of producing a model with the best possible benchmarks
	against datasets of samples, without taking into account if the results can be usefully
	interpreted, while existing explorations into making audio deepfake detection explainable have
	produced results that often require a lot of background knowledge to meaningfully interpret.
	Aside from this, explainability in the area of audio deepfake detection remains an open
	challenge \cite{cuccovillo_open_2022}, one even promoted by regularatory bodies such as the 
	European Union with the "right to explain" \cite{goodman_european_2017}. With the context of
	the research discussed in the next section, I would like to use new combinations of features
	with a novel model architecture to generate explanations for fake or real audio
	classification, with the goal of singling out features that are useful not only for an
	accurate classification but also for an understandable explanation.
	\section{Related Work}
	Several researchers have already followed similar or relevant directions to that of this work.
	This section will summarize some efforts in previous research categorized by the use of
	perceptible features or attempts to create explainable models.
	\subsection{Perceptible Features}
	The work of Barrington et al. \cite{barrington_single_2023} explores the idea of classification
	using perceptible features, highlighting their potential for improving explainability in the
	space of deepfake audio detection. They do, however also demonstrate a drop in performance in
	comparison with imperceptible hand-crafted and deep-learning features in their experiments,
	which were performed on a combination of synthetic and real audio datasets.
	\par
	Chaiwongyen et al. \cite{chaiwongyen_contribution_2022,chaiwongyen_deepfake-speech_2023} also
	approach using perceptible features for classification in their works. They trained and tested
	using the dataset of the ADD2022 Challenge \cite{yi_add_2024} and a simple model with a single
	hidden layer, resulting in lackluster performance in 2022, with better results using an
	expanded and improved feature set in 2023.
	\par
	Li et al. \cite{li_comparative_2022} also approach
	perceptible features in their work, combining them with imperceptible (referred to as physical
	in the work) features and using various neural networks in their experiments. Overall, the
	combination of perceptible and imperceptible features was able to produce the best performance
	results of the experiments, which were performed as well on the dataset of the ASV2022
	Challenge \cite{yi_add_2024}.
	\subsection{Explainable Models}
	One previous example of an implementation of an explainable model for use in audio deepfake
	detection came from Ge et al. \cite{ge_explaining_2024}, who explain feature influence on
	models using the SHAP (SHapley Additive exPlanations) method. They apply this method using
	log-scaled power spectrograms as a feature, training and testing using the datasets from the
	ASV2019 challenge \cite{wang_asvspoof_2020}, they are able to determine and graphically
	represent areas of importance on the spectrogram, as well as globally summarize the
	SHAP-values.
	\par
	Another example is presented by Haq et al. \cite{haq_multimodal_2023}, who use
	the changes in emotional state as an input feature and represent "unlikely" changes on a graph
	so that it can be understood by an eventual end user. In this case, they achieved their
	results by combining the output of fake video and fake audio classifiers in order to produce a
	final classification for a video sample with audio. Testing against the presidential deepfake
	dataset \cite{sankaranarayanan_presidential_nodate}, they achieve impressive results in
	comparison to the standing benchmark on the dataset at the time.
	\section{Experiments}
		\subsection{Dataset}
		The dataset used for these experiments is the "In-the-Wild" dataset, one which focuses on
		the generalization of audio deepfake detection models by collecting real-world data, in
		comparison to other examples that use more controlled laboratory conditions
		\cite{muller_does_2022}.
		\subsection{Black-Box Model}
		The model used for this experiment was made, trained and evaluated using the tools
		available in the TensorFlow and Keras libraries \cite{tensorflow2015-whitepaper}. This
		section will go into further detail about the experimental setup for the black-box model
		as well as the way in which explanations were created and summarized.
			\subsubsection{Features}
			Several features were used for the final version of the black-box model. Some are
			features that can be called generic, widely used in research involving the detection
			of synthetic while others are more specific and are cited accordingly. The features
			that are not summarized for a whole sample are extracted using a sliding window method,
			preventing a loss of fidelity that can be caused by compression of the feature to a
			standard size. The features used are as follows:
			\begin{itemize}
				\item
					\sloppy
					Harmonic-to-noise ratios: Inspired by previous research
					\cite{chaiwongyen_contribution_2022,chaiwongyen_deepfake-speech_2023,
					li_comparative_2022} but done in a sliding window fashion instead of on the
					whole file, the HNRs are the ratios of the strengths of harmonic frequencies
					to the strengths of "noise", the total strength outside the harmonic
					frequencies. Unlike some previously seen examples
					\cite{chaiwongyen_contribution_2022, chaiwongyen_deepfake-speech_2023}, I
					calculate this ratio for each fundamental frequency length, instead of using
					one value for the whole sample. Given that \(\gamma_{i}\) is the harmonic
					energy in a given fundamental frequency cycle and \(\iota_{i}\) is the
					residual energy in a given fundamental frequency cycle, the HNR at that cycle
					is calculated as follows:
					\[ 20log\frac{\gamma_{i}}{\iota_{i}} \]
				\item
					Mel-spectrograms: A widely-used feature in synthetic audio detection, the
					mel-spectrogram is a spectrographic representation of an audio sample
					transformed in a manner to better represent human perception of frequencies
					\cite{qais_deepfake_2022}.
				\item
					Mel-frequency cepstral coefficients: The derivative cepstral coefficients of
					the mel-spectrogram, also widely-seen in research on synthetic audio detection.
				\item
					Fundamental frequency lengths: The lengths of every fundamental frequency
					cycle in the sample. This has also been previously used in certain research on
					synthetic audio detection \cite{xue_audio_2022}. The output is one-dimensional
					with time as the axis.
				\item
					Onset strength: As used in previous research \cite{li_comparative_2022}, the
					strengths of each onset in the audio sample, where an onset is point where
					there is a sudden rise in energy across the audio spectrum. This results in a
					one-dimensional output with time as the axis.
				\item
					Intensity: Also inspired by previoous research \cite{li_comparative_2022},
					intensity is the total power at each point in the audio sample, given in db.
					This is calculated by creating a fourrier transformation of the sample and then
					summing across the frequency-axis for every point on the time-axis. Resulting
					in a one-dimensional output.
				\item
					Pitch-fluctuations: Calculated for every sample in the audio as the difference
					between pitch at the given point and the pitch at the previous point. Pitch is
					estimated based on the maximum power harmonics. This feature is similar to the
					use of summarized pitch fluctuations in previous research
					\cite{khanjani_learning_2023}. This feature is also one-dimensional with time
					as the axis. Given that \(H_{i}\) is the set of harmonic frequencies at
					fundamental frequency cycle \(i\) with \(h_{i} \in H_{i}\) as a frequency of
					the set and \(s(h_{i})\) is the power of a given harmonic frequency at cycle
					\(i\), the pitch can be estimated as follows:
					\[p_{i} = s(max(H_{i}))\]
					Then, given an offset \(x\), the pitch fluctuations can be calculated as
					follows:
					\[p_{i}-p_{i-x}\]
				\item
					Jitter features: As defined in previous attempts to identify synthetic audio
					\cite{chaiwongyen_deepfake-speech_2023} with perceptible features,
					jitter-based features measures the absolute variations in fundamental
					frequency cycles in comparison to the nearest x neighbors using the following
					method:
					\[ \frac{ \frac{1}{N-1}\sum_{i=1}^{N-1}|T_{i}(\frac{1}{x}\sum_{n=i-m}^{i+m}T_{n})|}
					{\frac{1}{N}\sum_{i=1}^{N}T_{i}} \]
					where \(T_{i}\) represents the extracted fundamental frequency lengths and
					\(N\) is the number of fundamental frequency periods.
				\item
					Shimmer features: Similarly defined to the jitter features, shimmer features
					instead use the amplitudes at each fundamental frequency period. Their purpose
					is to capture irregular vocal fold vibrations which may be an indication but
					not a guarantee of a synthetic voice \cite{chaiwongyen_deepfake-speech_2023}.
					\[ \frac{ \frac{1}{N-1}\sum_{i=1}^{N-1}|A_{i}(\frac{1}{x}\sum_{n=i-m}^{i+m}A_{n})|}
					{\frac{1}{N}\sum_{i=1}^{N}A_{i}} \]
			\end{itemize}
			\subsubsection{Model Architecture}
			The general architecture is designed to allow the combination of diverse features into
			one final prediction, regardless of whether the individual features have the same
			shape. This is achieved by using a separate model for each feature, with its own input
			and output layers (further referred to as sub-models), which are then concatenated
			together, processed though a final model that pools into a final output value (which I
			will further refer to as the terminus model). The modular structure allows not only
			better performance and less memory use, due to the fact that fewer transformations are
			required in input preparation, but also allows for more flexibility in the
			construction of the model, making it easy to add and remove features using pre-defined
			functions depending on feature type. This kind of architecture also provides an
			advantage to the generation of explanations, as only the terminus part of the entire
			must be explained to interpret the importance of each input feature. Further
			explanation is in the following section. Figure \ref{fig:model_plot} represents the
			layout of the model layers.
			\begin{figure}[htbp]
				\begin{center}
					\includegraphics[width=1\textwidth]{images/model_plot.png}
					\caption{The architecture of the model used for the experiment.}
					\label{fig:model_plot}
				\end{center}
			\end{figure}
			\subsubsection{Training and Evaluation}
			The black-box model described in the previous section was trained on the first //TODO//
			samples in the dataset. Evaluation was then performed on the remaining samples in the
			dataset. Training batches consisted of 100 samples each with every window position for
			each sample and an upper limit of 1,000,000 lines per batch. For each batch, two
			epochs were performed. Because a single sample can have multiple input lines,
			evaluation cannot be performed on a per-line basis, but must instead be summarized.
			Two methods are possible: median and mean, with the difference in results being
			negligible in my experiments. The default threshold is 0.5, meaning that a result
			under 0.5 is a false result and over 0.5 is a true result. This threshold can however
			be adjusted, useful for computing the EER.
		\subsection{Explanations}
		The explanations using the previously described surrogate model are generated with the
		Local Interpretable Model-agnostic Explanation (LIME) method \cite{ribeiro_why_2016}. This
		section will describe in detail how individual explanations are generated at a local level
		for individual samples, as well as how the usefulness of the explanations can be evaluated
		in general.
			\subsubsection{Generation of the Explanations}
			Because of the multi-shaped nature and independence of the input layers of the model,
			the explanations cannot be generated directly from the input itself. However, because
			the sub-models are separate from one another, having no influence on each other before
			being concatenated at the terminus, only the terminus part of the model is relevant
			for assessing the importance of the features in a single evaluation. This does,
			however, present two challenges:
			\begin{itemize}
				\item There are multiple input rows per sample.
				\item The features cannot be used directly for assessment at the terminus input
					layer.
			\end{itemize}
			The first problem can be solved by taking the mean of the explanation weights of each
			feature produced by each row of the sample features. This results in an average
			contribution of each feature to the classification of the model. Because the
			end-classification of the surrogate is also a mean, the means of the LIME-results are
			an accurate representation of the aggregate influence.
			\par
			I approached the second problem by generating intermediate data for the
			LIME-evaluation. Intermediate data was generated by first decomposing the surrogate
			model into its component models, the sub-models and the terminus, and for each
			sub-model, running a prediction on the given input features. The results of the
			predictions of the sub-models were stored and used for further analysis. This was done
			not only for the sample explanation data but also for the training data, to use as an
			input for the LIME explainer.
			\par
			The results of the LIME explainer are then summarized together on a per-feature basis
			and normalized to produce a decimal number between -1 and 1 for every feature. In this
			case, a negative value implies that the given feature pushed the result of the model
			in the negative direction (i.e. not fake), while a positive value indicates the
			opposite.
			\subsubsection{Evaluation of the Explanations}
			In order to assess if the explanations have the potential to be useful to an end
			user, I pursued the goal of summarizing many explanations generated using the
			previously described method together in a meaningful way, in order to make a
			conclusion about the general usefulness of the individual features and the influence
			and potential use of the perceptible features. However, due to a lack of existing
			standardized metrics for the global evaluation of local metrics, I propose two
			metrics, with which I will make conclusions about the usefulness of this method.
			\par
			In order to contextualize the metrics, I will make the following definitions.
			\begin{itemize}
				\item Let \(S\) be the set of samples with \(s \in S\) as an element of the set.
				\item Let \(F\) be the set of features with \(f \in F\) as an element of the set.
				\item Let \(w_{fs}\) be the weight value of feature \(f\) of sample \(s\) as
					produced by the explainer.
				\item Let \(l_{s}\) be the correct label of the sample \(s\).
			\end{itemize}
			The first metric I will define is the mean of the absolute values of the weights of
			the explanations, summarized along the sample axis, delivering one value per feature.
			I will further refer to this metric as importance, and it can be mathematically
			defined as follows:
			\[\frac{\sum_{s \in S} |w_{fs}|}{|S|}\]
			The second metric I will define is the average aggregate correctness on a per-feature
			basis, which I will further refer to as trust. The trust per feature can be defined as
			follows:
			\[\frac{\sum_{s \in S} w_{fs}(2l_{s}-1)}{|S|}\]
	\section{Results}
		\subsection{Black-Box Model Results}
		\subsection{Explanation Results}
	\section{Discussion}
	\section{Conclusion}
	\newpage
	\sloppy
	\printbibliography
\end{document}
