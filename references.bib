@article{veerasamy_rising_2022,
	title = {Rising Above Misinformation and Deepfakes},
	volume = {17},
	rights = {https://creativecommons.org/licenses/by-nc-nd/4.0},
	issn = {2048-9889, 2048-9870},
	url = {https://papers.academic-conferences.org/index.php/iccws/article/view/25},
	doi = {10.34190/iccws.17.1.25},
	abstract = {Misinformation can be rapidly spread in cyberspace. It thrives in the social media landscape as well as news platforms. Misinformation can readily gain momentum in the race to influence people or intentionally deceive. With the use of bots, misinformation can be easily shared, especially in environments like Twitter and Facebook. While, some measures are taken to stop the spread of misinformation, threats like Deepfakes are posing a higher challenge. Deepfakes provide a means to generate fake digital content in order to impersonate a person. With the use of audio, images and videos, artificial intelligence is used to depict the speech and actions of people. Deepfakes are typically made of presidents or influential businessmen such as Donald Trump and Mark Zuckerberg. Deep Fakes can be very realistic and convincing as this form of synthetic media is raising concerns about its possible misuse. The effects of Deepfakes are to spread disinformation, confuse users or create influence. This can lead to further effects like political factions, blackmail, harassment and extortion. Deepfakes could lead to a distrust in digital content as many may feel that anything we see is actually just a manipulation. Deepfakes has arisen as a new generation of misinformation through the manipulation of digital media in order to create realistic videos. This paper looks at the governing, communal and technical issues relating to Deepfakes. At the technical level, the use of audio and text analysis used to create Deepfake videos is advancing at a rapid pace which has also made its affordability and accessibility easier. An evaluation of the threats stemming from Deepfakes reveals that there are various mental, monetary and group dynamics involved. In this paper, the various types of threats emanating from Deepfakes is discussed. This paper also looks at five factors in the field of Deepfakes that should be taken into consideration (Technical Source Dissemination Victim Viewers). The paper discussed these five factors in order to help identify measures to help curb the spread of Deepfakes. A combination of these measures can help limit the spread of Deepfakes and support mitigation of the threat. Due to prominence and power that digital media has, it is imperative that this threat not be overlooked. The paper provides a holistic approach to understanding the risk and impact of Deepfakes, as well measures to help mitigate abuse thereof.},
	pages = {340--348},
	number = {1},
	journaltitle = {International Conference on Cyber Warfare and Security},
	shortjournal = {iccws},
	author = {Veerasamy, Namosha and Pieterse, Heloise},
	urldate = {2024-09-23},
	date = {2022-03-02},
	langid = {english},
	file = {PDF:/home/jacob/Zotero/storage/CQRFPGQR/Veerasamy and Pieterse - 2022 - Rising Above Misinformation and Deepfakes.pdf:application/pdf},
}

@article{albahar_deepfakes_2005,
	title = {{DEEPFAKES}: {THREATS} {AND} {COUNTERMEASURES} {SYSTEMATIC} {REVIEW}},
	abstract = {Deepfake, a machine learning-based software tool, has made it easy to alter or manipulate images and videos. Images are frequently used as evidence in investigations and in court. However, technological developments, and deepfake in particular, have potentially made these pieces of evidence unreliable. Altered images and videos are not only surprisingly convincing but are also difficult to identify as fake or real. Deepfakes have been used to blackmail, fake terrorism events, disseminate fake news, defame individuals, and to create political distress. To gain in-depth insight into the deepfake technology, the present research examines its origin and history while assessing how deepfake videos and photos are created. Moreover, the research also focuses on the impact deepfake has made on society in terms of how it has been applied. Different methods have been developed for detecting deepfakes including face detection, multimedia forensics, watermarking, and convolutional neural networks ({CNNs}). Each method uses machine learning, a technique from the field of artificial intelligence, to detect any kind of manipulation in photos and videos.},
	number = {22},
	journaltitle = {. Vol.},
	author = {Albahar, Marwan and Almalki, Jameel},
	date = {2005},
	langid = {english},
	file = {PDF:/home/jacob/Zotero/storage/8VKR24HG/Albahar and Almalki - 2005 - DEEPFAKES THREATS AND COUNTERMEASURES SYSTEMATIC REVIEW.pdf:application/pdf},
}

@article{hind_explaining_2019,
	title = {Explaining explainable {AI}},
	volume = {25},
	issn = {1528-4972, 1528-4980},
	url = {https://dl.acm.org/doi/10.1145/3313096},
	doi = {10.1145/3313096},
	abstract = {How good are you at explaining your decisions? Are you better than a machine? Today, {AI} systems are being asked to explain their decisions. This article explores the challenges in solving this problem and approaches researchers are pursuing.},
	pages = {16--19},
	number = {3},
	journaltitle = {{XRDS}: Crossroads, The {ACM} Magazine for Students},
	shortjournal = {{XRDS}},
	author = {Hind, Michael},
	urldate = {2024-06-05},
	date = {2019-04-10},
	langid = {english},
	file = {Hind - 2019 - Explaining explainable AI.pdf:/home/jacob/Zotero/storage/MGIAIRCD/Hind - 2019 - Explaining explainable AI.pdf:application/pdf},
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@inproceedings{chaiwongyen_contribution_2022,
	location = {Chiang Mai, Thailand},
	title = {Contribution of Timbre and Shimmer Features to Deepfake Speech Detection},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {978-616-590-477-3},
	url = {https://ieeexplore.ieee.org/document/9980281/},
	doi = {10.23919/APSIPAASC55919.2022.9980281},
	abstract = {ÐAdvanced deep-learning techniques can generate natural and synthetic voices that might be close to someone’s voice. Nevertheless, misuse of such technologies is of great concern. Hence, researchers focus on detecting these malicious synthetic voices, called ªdeepfake speech.º Although many feature extractions and classifications have been proposed, the accuracy of deepfake detection is still unreliable. In addition, most of the current features are computed in the frequency domain. To this end, we conducted experiments to investigate the contribution of two acoustic features and deepfake speech signals. The acoustic features are timbre and shimmer, which represent our auditory perception in the time domain. We point out that eight timbre components and four shimmer components significantly contribute to discriminating deepfake speech from genuine speech. We also propose a method for detecting deepfake speech based on these timbre and shimmer features. The method was evaluated by using a dataset from the Audio Deep Synthesis Detection Challenge ({ADD} 2022). The results suggest that combining these eight timbre components and four shimmer components with a simple classifier using multilayer perceptron neural networks can enable deepfake speech to be detected potentially effectively.},
	eventtitle = {2022 Asia Pacific Signal and Information Processing Association Annual Summit and Conference ({APSIPA} {ASC})},
	pages = {97--103},
	booktitle = {2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference ({APSIPA} {ASC})},
	publisher = {{IEEE}},
	author = {Chaiwongyen, Anuwat and Songsriboonsit, Norranat and Duangpummet, Suradej and Karnjana, Jessada and Kongprawechnon, Waree and Unoki, Masashi},
	urldate = {2024-05-28},
	date = {2022-11-07},
	langid = {english},
	file = {Chaiwongyen et al. - 2022 - Contribution of Timbre and Shimmer Features to Dee.pdf:/home/jacob/Zotero/storage/PK9YUPJB/Chaiwongyen et al. - 2022 - Contribution of Timbre and Shimmer Features to Dee.pdf:application/pdf},
}

@inproceedings{chaiwongyen_deepfake-speech_2023,
	location = {Taipei, Taiwan},
	title = {Deepfake-speech Detection with Pathological Features and Multilayer Perceptron Neural Network},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {9798350300673},
	url = {https://ieeexplore.ieee.org/document/10317331/},
	doi = {10.1109/APSIPAASC58517.2023.10317331},
	abstract = {Deepfake speech, a misuse of speech technology, is of great concern since it seems natural and is difﬁcult to detect. Although many methods using various speech features have been proposed, deepfake-speech detection accuracy must be improved, especially in real-world scenarios. Therefore, this paper presents a method for detecting deepfake speech on the basis of pathological features used by pathologists for assessing voice quality. The six-pathological features, including jitter, shimmer, harmonicsto-noise ratio, cepstral-harmonics-to-noise ratio, normalized noise energy, and glottal-to-noise excitation ratio, are fed to a multilayer perceptron neural network. We evaluated the proposed method using the Audio Deep Synthesis Detection Challenge dataset. The results indicate that the proposed model can be used for detecting deepfake speech. The proposed method’s accuracy, precision, recall, and F1-score were over 98\% on the development set, and it outperformed the baseline method on the adaptation set.},
	eventtitle = {2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference ({APSIPA} {ASC})},
	pages = {2182--2188},
	booktitle = {2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference ({APSIPA} {ASC})},
	publisher = {{IEEE}},
	author = {Chaiwongyen, Anuwat and Duangpummet, Suradej and Karnjana, Jessada and Kongprawechnon, Waree and Unoki, Masashi},
	urldate = {2024-05-28},
	date = {2023-10-31},
	langid = {english},
	file = {Chaiwongyen et al. - 2023 - Deepfake-speech Detection with Pathological Featur.pdf:/home/jacob/Zotero/storage/JPYE4FYL/Chaiwongyen et al. - 2023 - Deepfake-speech Detection with Pathological Featur.pdf:application/pdf},
}

@inproceedings{li_comparative_2022,
	location = {Lisboa Portugal},
	title = {A Comparative Study on Physical and Perceptual Features for Deepfake Audio Detection},
	isbn = {978-1-4503-9496-3},
	url = {https://dl.acm.org/doi/10.1145/3552466.3556523},
	doi = {10.1145/3552466.3556523},
	abstract = {Audio content synthesis has stepped into a new era and brought a great threat to daily life since the development of deep learning techniques. The {ASVSpoof} Challenge and the {ADD} Challenge have been launched to motivate the development of Deepfake audio detection algorithms. Currently, the detection models, which consist of front-end feature extractors and back-end classifiers, utilize the physical features mainly, rather than the perceptual features that relate to natural emotions or breathiness. Therefore, we provide a comprehensive study on 16 physical and perceptual features and evaluate their effectiveness in both Track 1 and Track 2 of the {ADD} Challenge. Based on results, {PLP}, as a perceptual feature, outperforms the rest of the features in Track 1, while {CQCC} has the best performance in Track 2. Our experiments demonstrate the significance of perceptual features in detecting Deepfake audios. We also seek to explore the underlying characteristics of features that can distinguish Deepfake audio from a real one. We perform statistical analysis on each feature to show its distribution differences on real and synthesized audios. This paper will provide a potential direction in selecting appropriate feature extraction methods for the future implementation of detection models.},
	eventtitle = {{MM} '22: The 30th {ACM} International Conference on Multimedia},
	pages = {35--41},
	booktitle = {Proceedings of the 1st International Workshop on Deepfake Detection for Audio Multimedia},
	publisher = {{ACM}},
	author = {Li, Menglu and Ahmadiadli, Yasaman and Zhang, Xiao-Ping},
	urldate = {2024-05-11},
	date = {2022-10-14},
	langid = {english},
	file = {Li et al. - 2022 - A Comparative Study on Physical and Perceptual Fea.pdf:/home/jacob/Zotero/storage/RPQU57UG/Li et al. - 2022 - A Comparative Study on Physical and Perceptual Fea.pdf:application/pdf},
}

@inproceedings{qais_deepfake_2022,
	location = {Hyderabad, India},
	title = {Deepfake Audio Detection with Neural Networks Using Audio Features},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66547-258-6},
	url = {https://ieeexplore.ieee.org/document/9862519/},
	doi = {10.1109/ICICCSP53532.2022.9862519},
	abstract = {In this paper, a speech spoofing detection system based on Convolutional neural networks using different audio features has been proposed to classify the human speech and synthetic voice, Worst-case scenarios can develop using deepfake audios as threat to assets and image of a person, it can also become a threat to the whole country by unethical uses intended for loss of other party. Using a small voice clip of a person an attacker can develop similar voices. Every audio signal can be represented on a 2D graph plotted by mathematical calculations. The processing of audios into {CNN} requires a lot of computation, to make a system that can detect deepfake voices with much less computation by conversion of audios to images of audio features (Spectrogram, {MFCC}, {FFT}, {STFT} ) and then obtaining the array values as a numeric format which are most suitable to feed. Different approaches for feeding data to model are applied for prediction individually as well as in a concatenated approach.},
	eventtitle = {2022 International Conference on Intelligent Controller and Computing for Smart Power ({ICICCSP})},
	pages = {1--6},
	booktitle = {2022 International Conference on Intelligent Controller and Computing for Smart Power ({ICICCSP})},
	publisher = {{IEEE}},
	author = {Qais, Abu and Rastogi, Akshar and Saxena, Akash and Rana, Arpit and Sinha, Deependra},
	urldate = {2024-05-28},
	date = {2022-07-21},
	langid = {english},
	file = {Qais et al. - 2022 - Deepfake Audio Detection with Neural Networks Usin.pdf:/home/jacob/Zotero/storage/U4Z3ZRM3/Qais et al. - 2022 - Deepfake Audio Detection with Neural Networks Usin.pdf:application/pdf},
}

@inproceedings{xue_audio_2022,
	location = {Lisboa Portugal},
	title = {Audio Deepfake Detection Based on a Combination of F0 Information and Real Plus Imaginary Spectrogram Features},
	isbn = {978-1-4503-9496-3},
	url = {https://dl.acm.org/doi/10.1145/3552466.3556526},
	doi = {10.1145/3552466.3556526},
	eventtitle = {{MM} '22: The 30th {ACM} International Conference on Multimedia},
	pages = {19--26},
	booktitle = {Proceedings of the 1st International Workshop on Deepfake Detection for Audio Multimedia},
	publisher = {{ACM}},
	author = {Xue, Jun and Fan, Cunhang and Lv, Zhao and Tao, Jianhua and Yi, Jiangyan and Zheng, Chengshi and Wen, Zhengqi and Yuan, Minmin and Shao, Shegang},
	urldate = {2024-05-11},
	date = {2022-10-14},
	langid = {english},
	file = {Xue et al. - 2022 - Audio Deepfake Detection Based on a Combination of.pdf:/home/jacob/Zotero/storage/H7BIS2GL/Xue et al. - 2022 - Audio Deepfake Detection Based on a Combination of.pdf:application/pdf},
}

@inproceedings{khanjani_learning_2023,
	location = {Charlotte, {NC}, {USA}},
	title = {Learning to Listen and Listening to Learn: Spoofed Audio Detection Through Linguistic Data Augmentation},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {9798350337730},
	url = {https://ieeexplore.ieee.org/document/10297267/},
	doi = {10.1109/ISI58743.2023.10297267},
	shorttitle = {Learning to Listen and Listening to Learn},
	abstract = {Spoofed audio, both human or machine generated, causes deception and disinformation and as such is a societal challenge. This study advances the detection of spoofed audio through a novel approach that augments knowledge about audio data by incorporating linguistic information. Using perceptual methods, for English audio samples, experts in sociolinguistics listened for audio cues, and used binary labels to indicate the perceived authenticity of a set of speech samples, based on phonetic and phonological features that occur frequently in spoken English. These Expert Defined Linguistic Features ({EDLFs}) were then used in supervised spoofed audio detection methods to augment {AI} models. An ensemble method based on multi-domain features both from the audio data itself and the {EDLFs} was also created to evaluate the spoofed audio detection, and to demonstrate how {EDLFs} can improve traditional methods of spoofed audio detection. We found that augmenting the audio data with expertinformed linguistic annotation increased the accuracy of spoofed audio detection significantly in both the training and testing datasets across the evaluated single and ensemble models. Our findings indicate the promising avenue of augmenting audio data with perceptual linguistic techniques, as a method of human discernment, to enhance {AI}-based approaches for spoofed audio detection. These features also establish a foundation for direct linguistic annotations on new audio clips for robust spoofed audio detection.},
	eventtitle = {2023 {IEEE} International Conference on Intelligence and Security Informatics ({ISI})},
	pages = {01--06},
	booktitle = {2023 {IEEE} International Conference on Intelligence and Security Informatics ({ISI})},
	publisher = {{IEEE}},
	author = {Khanjani, Zahra and Davis, Lavon and Tuz, Anna and Nwosu, Kifekachukwu and Mallinson, Christine and Janeja, Vandana P.},
	urldate = {2024-05-28},
	date = {2023-10-02},
	langid = {english},
	file = {Khanjani et al. - 2023 - Learning to Listen and Listening to Learn Spoofed.pdf:/home/jacob/Zotero/storage/MDJK3BG3/Khanjani et al. - 2023 - Learning to Listen and Listening to Learn Spoofed.pdf:application/pdf},
}

@misc{muller_does_2022,
	title = {Does Audio Deepfake Detection Generalize?},
	url = {http://arxiv.org/abs/2203.16263},
	abstract = {Current text-to-speech algorithms produce realistic fakes of human voices, making deepfake detection a much-needed area of research. While researchers have presented various deep learning models for audio spoofs detection, it is often unclear exactly why these architectures are successful: Preprocessing steps, hyperparameter settings, and the degree of ﬁne-tuning are not consistent across related work. Which factors contribute to success, and which are accidental? In this work, we address this problem: We systematize audio spooﬁng detection by re-implementing and uniformly evaluating twelve architectures from related work. We identify overarching features for successful audio deepfake detection, such as using cqtspec or logspec features instead of melspec features, which improves performance by 37\% {EER} on average, all other factors constant.},
	number = {{arXiv}:2203.16263},
	publisher = {{arXiv}},
	author = {Müller, Nicolas M. and Czempin, Pavel and Dieckmann, Franziska and Froghyar, Adam and Böttinger, Konstantin},
	urldate = {2024-07-04},
	date = {2022-04-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2203.16263 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {Müller et al. - 2022 - Does Audio Deepfake Detection Generalize.pdf:/home/jacob/Zotero/storage/YBAF5V2M/Müller et al. - 2022 - Does Audio Deepfake Detection Generalize.pdf:application/pdf},
}

@misc{barrington_single_2023,
	title = {Single and Multi-Speaker Cloned Voice Detection: From Perceptual to Learned Features},
	url = {http://arxiv.org/abs/2307.07683},
	doi = {10.48550/arXiv.2307.07683},
	shorttitle = {Single and Multi-Speaker Cloned Voice Detection},
	abstract = {Synthetic-voice cloning technologies have seen significant advances in recent years, giving rise to a range of potential harms. From small- and large-scale financial fraud to disinformation campaigns, the need for reliable methods to differentiate real and synthesized voices is imperative. We describe three techniques for differentiating a real from a cloned voice designed to impersonate a specific person. These three approaches differ in their feature extraction stage with lowdimensional perceptual features offering high interpretability but lower accuracy, to generic spectral features, and end-to-end learned features offering less interpretability but higher accuracy. We show the efficacy of these approaches when trained on a single speaker’s voice and when trained on multiple voices. The learned features consistently yield an equal error rate between 0\% and 4\%, and are reasonably robust to adversarial laundering.},
	number = {{arXiv}:2307.07683},
	publisher = {{arXiv}},
	author = {Barrington, Sarah and Barua, Romit and Koorma, Gautham and Farid, Hany},
	urldate = {2024-12-18},
	date = {2023-09-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2307.07683 [cs]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language},
	file = {PDF:/home/jacob/Zotero/storage/EIMG2L3I/Barrington et al. - 2023 - Single and Multi-Speaker Cloned Voice Detection From Perceptual to Learned Features.pdf:application/pdf},
}

@misc{ge_explaining_2024,
	title = {Explaining deep learning models for spoofing and deepfake detection with {SHapley} Additive {exPlanations}},
	url = {http://arxiv.org/abs/2110.03309},
	doi = {10.48550/arXiv.2110.03309},
	abstract = {Substantial progress in spoofing and deepfake detection has been made in recent years. Nonetheless, the community has yet to make notable inroads in providing an explanation for how a classifier produces its output. The dominance of black box spoofing detection solutions is at further odds with the drive toward trustworthy, explainable artificial intelligence. This paper describes our use of {SHapley} Additive {exPlanations} ({SHAP}) to gain new insights in spoofing detection. We demonstrate use of the tool in revealing unexpected classifier behaviour, the artefacts that contribute most to classifier outputs and differences in the behaviour of competing spoofing detection models. The tool is both efficient and flexible, being readily applicable to a host of different architecture models in addition to related, different applications. All results reported in the paper are reproducible using open-source software.},
	number = {{arXiv}:2110.03309},
	publisher = {{arXiv}},
	author = {Ge, Wanying and Patino, Jose and Todisco, Massimiliano and Evans, Nicholas},
	urldate = {2024-12-18},
	date = {2024-04-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2110.03309 [eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {PDF:/home/jacob/Zotero/storage/9R26CPNM/Ge et al. - 2024 - Explaining deep learning models for spoofing and deepfake detection with SHapley Additive exPlanatio.pdf:application/pdf},
}

@article{haq_multimodal_2023,
	title = {Multimodal Neurosymbolic Approach for Explainable Deepfake Detection},
	issn = {1551-6857, 1551-6865},
	url = {https://dl.acm.org/doi/10.1145/3624748},
	doi = {10.1145/3624748},
	abstract = {Deepfake detection has become increasingly important in recent years owing to the widespread availability of deepfake generation technologies. Existing deepfake detection methods present two primary limitations i.e., trained on a specific type of deepfake dataset, which renders them vulnerable to unseen deepfakes; and they regard deepfakes as a black-box with limited explainability, making it difficult for non-{AI} experts to understand and trust the decisions. Hence, this paper proposes a novel neurosymbolic deepfake detection framework that exploits the fact that human emotions cannot be imitated easily owing to their complex nature. We argue that deep fakes typically exhibit inter- or intra- modality inconsistencies in the emotional expressions of the person being manipulated. Thus, the proposed framework performs inter- and intra- modality reasoning on emotions extracted from audio and visual modalities using a psychological and arousalvalence model for deepfake detection. In addition to fake detection, the proposed framework provides textual explanations for its decisions. The results obtained using Presidential Deepfakes Dataset and World Leaders Dataset of real and manipulated videos demonstrate the effectiveness of our approach in detecting deepfakes and highlight the potential of neurosymbolic approach for expandability.},
	pages = {3624748},
	journaltitle = {{ACM} Transactions on Multimedia Computing, Communications, and Applications},
	shortjournal = {{ACM} Trans. Multimedia Comput. Commun. Appl.},
	author = {Haq, Ijaz Ul and Malik, Khalid Mahmood and Muhammad, Khan},
	urldate = {2024-05-11},
	date = {2023-09-20},
	langid = {english},
	file = {Haq et al. - 2023 - Multimodal Neurosymbolic Approach for Explainable .pdf:/home/jacob/Zotero/storage/2XAC7A5W/Haq et al. - 2023 - Multimodal Neurosymbolic Approach for Explainable .pdf:application/pdf},
}

@inproceedings{cuccovillo_open_2022,
	title = {Open Challenges in Synthetic Speech Detection},
	url = {http://arxiv.org/abs/2209.07180},
	doi = {10.1109/WIFS55849.2022.9975433},
	abstract = {In this paper the current status and open challenges of synthetic speech detection are addressed. The work comprises an initial analysis of available open datasets and of existing detection methods, a description of the requirements for new research datasets compliant with regulations and better representing real-case scenarios, and a discussion of the desired characteristics of future trustworthy detection methods in terms of both functional and non-functional requirements. Compared to other works, based on speciﬁc detection solutions or presenting single dataset of synthetic speeches, our paper is meant to orient future state-of-the-art research in the domain, to quickly lessen the current gap between synthesis and detection approaches.},
	pages = {1--6},
	booktitle = {2022 {IEEE} International Workshop on Information Forensics and Security ({WIFS})},
	author = {Cuccovillo, Luca and Papastergiopoulos, Christoforos and Vafeiadis, Anastasios and Yaroshchuk, Artem and Aichroth, Patrick and Votis, Konstantinos and Tzovaras, Dimitrios},
	urldate = {2024-12-18},
	date = {2022-12-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2209.07180 [eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {PDF:/home/jacob/Zotero/storage/93FTPZLY/Cuccovillo et al. - 2022 - Open Challenges in Synthetic Speech Detection.pdf:application/pdf},
}

@article{goodman_european_2017,
	title = {European Union regulations on algorithmic decision-making and a "right to explanation"},
	volume = {38},
	issn = {0738-4602, 2371-9621},
	url = {http://arxiv.org/abs/1606.08813},
	doi = {10.1609/aimag.v38i3.2741},
	abstract = {We summarize the potential impact that the European Union’s new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the {EU} in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on userlevel predictors) which “signiﬁcantly affect” users. The law will also effectively create a “right to explanation,” whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for computer scientists to take the lead in designing algorithms and evaluation frameworks which avoid discrimination and enable explanation.},
	pages = {50--57},
	number = {3},
	journaltitle = {{AI} Magazine},
	shortjournal = {{AI} Magazine},
	author = {Goodman, Bryce and Flaxman, Seth},
	urldate = {2025-01-18},
	date = {2017-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1606.08813 [stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computers and Society, Statistics - Machine Learning},
	file = {PDF:/home/jacob/Zotero/storage/92FH2PJU/Goodman and Flaxman - 2017 - European Union regulations on algorithmic decision-making and a right to explanation.pdf:application/pdf},
}
