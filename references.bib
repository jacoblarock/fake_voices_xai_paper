@article{veerasamy_rising_2022,
	title = {Rising Above Misinformation and Deepfakes},
	volume = {17},
	rights = {https://creativecommons.org/licenses/by-nc-nd/4.0},
	issn = {2048-9889, 2048-9870},
	url = {https://papers.academic-conferences.org/index.php/iccws/article/view/25},
	doi = {10.34190/iccws.17.1.25},
	abstract = {Misinformation can be rapidly spread in cyberspace. It thrives in the social media landscape as well as news platforms. Misinformation can readily gain momentum in the race to influence people or intentionally deceive. With the use of bots, misinformation can be easily shared, especially in environments like Twitter and Facebook. While, some measures are taken to stop the spread of misinformation, threats like Deepfakes are posing a higher challenge. Deepfakes provide a means to generate fake digital content in order to impersonate a person. With the use of audio, images and videos, artificial intelligence is used to depict the speech and actions of people. Deepfakes are typically made of presidents or influential businessmen such as Donald Trump and Mark Zuckerberg. Deep Fakes can be very realistic and convincing as this form of synthetic media is raising concerns about its possible misuse. The effects of Deepfakes are to spread disinformation, confuse users or create influence. This can lead to further effects like political factions, blackmail, harassment and extortion. Deepfakes could lead to a distrust in digital content as many may feel that anything we see is actually just a manipulation. Deepfakes has arisen as a new generation of misinformation through the manipulation of digital media in order to create realistic videos. This paper looks at the governing, communal and technical issues relating to Deepfakes. At the technical level, the use of audio and text analysis used to create Deepfake videos is advancing at a rapid pace which has also made its affordability and accessibility easier. An evaluation of the threats stemming from Deepfakes reveals that there are various mental, monetary and group dynamics involved. In this paper, the various types of threats emanating from Deepfakes is discussed. This paper also looks at five factors in the field of Deepfakes that should be taken into consideration (Technical Source Dissemination Victim Viewers). The paper discussed these five factors in order to help identify measures to help curb the spread of Deepfakes. A combination of these measures can help limit the spread of Deepfakes and support mitigation of the threat. Due to prominence and power that digital media has, it is imperative that this threat not be overlooked. The paper provides a holistic approach to understanding the risk and impact of Deepfakes, as well measures to help mitigate abuse thereof.},
	pages = {340--348},
	number = {1},
	journaltitle = {International Conference on Cyber Warfare and Security},
	shortjournal = {iccws},
	author = {Veerasamy, Namosha and Pieterse, Heloise},
	urldate = {2024-09-23},
	date = {2022-03-02},
	langid = {english},
	file = {PDF:/home/jacob/Zotero/storage/CQRFPGQR/Veerasamy and Pieterse - 2022 - Rising Above Misinformation and Deepfakes.pdf:application/pdf},
}

@article{albahar_deepfakes_2005,
	title = {{DEEPFAKES}: {THREATS} {AND} {COUNTERMEASURES} {SYSTEMATIC} {REVIEW}},
	abstract = {Deepfake, a machine learning-based software tool, has made it easy to alter or manipulate images and videos. Images are frequently used as evidence in investigations and in court. However, technological developments, and deepfake in particular, have potentially made these pieces of evidence unreliable. Altered images and videos are not only surprisingly convincing but are also difficult to identify as fake or real. Deepfakes have been used to blackmail, fake terrorism events, disseminate fake news, defame individuals, and to create political distress. To gain in-depth insight into the deepfake technology, the present research examines its origin and history while assessing how deepfake videos and photos are created. Moreover, the research also focuses on the impact deepfake has made on society in terms of how it has been applied. Different methods have been developed for detecting deepfakes including face detection, multimedia forensics, watermarking, and convolutional neural networks ({CNNs}). Each method uses machine learning, a technique from the field of artificial intelligence, to detect any kind of manipulation in photos and videos.},
	number = {22},
	journaltitle = {. Vol.},
	author = {Albahar, Marwan and Almalki, Jameel},
	date = {2005},
	langid = {english},
	file = {PDF:/home/jacob/Zotero/storage/8VKR24HG/Albahar and Almalki - 2005 - DEEPFAKES THREATS AND COUNTERMEASURES SYSTEMATIC REVIEW.pdf:application/pdf},
}

@article{hind_explaining_2019,
	title = {Explaining explainable {AI}},
	volume = {25},
	issn = {1528-4972, 1528-4980},
	url = {https://dl.acm.org/doi/10.1145/3313096},
	doi = {10.1145/3313096},
	abstract = {How good are you at explaining your decisions? Are you better than a machine? Today, {AI} systems are being asked to explain their decisions. This article explores the challenges in solving this problem and approaches researchers are pursuing.},
	pages = {16--19},
	number = {3},
	journaltitle = {{XRDS}: Crossroads, The {ACM} Magazine for Students},
	shortjournal = {{XRDS}},
	author = {Hind, Michael},
	urldate = {2024-06-05},
	date = {2019-04-10},
	langid = {english},
	file = {Hind - 2019 - Explaining explainable AI.pdf:/home/jacob/Zotero/storage/MGIAIRCD/Hind - 2019 - Explaining explainable AI.pdf:application/pdf},
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@inproceedings{chaiwongyen_contribution_2022,
	location = {Chiang Mai, Thailand},
	title = {Contribution of Timbre and Shimmer Features to Deepfake Speech Detection},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {978-616-590-477-3},
	url = {https://ieeexplore.ieee.org/document/9980281/},
	doi = {10.23919/APSIPAASC55919.2022.9980281},
	abstract = {ÐAdvanced deep-learning techniques can generate natural and synthetic voices that might be close to someone’s voice. Nevertheless, misuse of such technologies is of great concern. Hence, researchers focus on detecting these malicious synthetic voices, called ªdeepfake speech.º Although many feature extractions and classifications have been proposed, the accuracy of deepfake detection is still unreliable. In addition, most of the current features are computed in the frequency domain. To this end, we conducted experiments to investigate the contribution of two acoustic features and deepfake speech signals. The acoustic features are timbre and shimmer, which represent our auditory perception in the time domain. We point out that eight timbre components and four shimmer components significantly contribute to discriminating deepfake speech from genuine speech. We also propose a method for detecting deepfake speech based on these timbre and shimmer features. The method was evaluated by using a dataset from the Audio Deep Synthesis Detection Challenge ({ADD} 2022). The results suggest that combining these eight timbre components and four shimmer components with a simple classifier using multilayer perceptron neural networks can enable deepfake speech to be detected potentially effectively.},
	eventtitle = {2022 Asia Pacific Signal and Information Processing Association Annual Summit and Conference ({APSIPA} {ASC})},
	pages = {97--103},
	booktitle = {2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference ({APSIPA} {ASC})},
	publisher = {{IEEE}},
	author = {Chaiwongyen, Anuwat and Songsriboonsit, Norranat and Duangpummet, Suradej and Karnjana, Jessada and Kongprawechnon, Waree and Unoki, Masashi},
	urldate = {2024-05-28},
	date = {2022-11-07},
	langid = {english},
	file = {Chaiwongyen et al. - 2022 - Contribution of Timbre and Shimmer Features to Dee.pdf:/home/jacob/Zotero/storage/PK9YUPJB/Chaiwongyen et al. - 2022 - Contribution of Timbre and Shimmer Features to Dee.pdf:application/pdf},
}

@inproceedings{chaiwongyen_deepfake-speech_2023,
	location = {Taipei, Taiwan},
	title = {Deepfake-speech Detection with Pathological Features and Multilayer Perceptron Neural Network},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {9798350300673},
	url = {https://ieeexplore.ieee.org/document/10317331/},
	doi = {10.1109/APSIPAASC58517.2023.10317331},
	abstract = {Deepfake speech, a misuse of speech technology, is of great concern since it seems natural and is difﬁcult to detect. Although many methods using various speech features have been proposed, deepfake-speech detection accuracy must be improved, especially in real-world scenarios. Therefore, this paper presents a method for detecting deepfake speech on the basis of pathological features used by pathologists for assessing voice quality. The six-pathological features, including jitter, shimmer, harmonicsto-noise ratio, cepstral-harmonics-to-noise ratio, normalized noise energy, and glottal-to-noise excitation ratio, are fed to a multilayer perceptron neural network. We evaluated the proposed method using the Audio Deep Synthesis Detection Challenge dataset. The results indicate that the proposed model can be used for detecting deepfake speech. The proposed method’s accuracy, precision, recall, and F1-score were over 98\% on the development set, and it outperformed the baseline method on the adaptation set.},
	eventtitle = {2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference ({APSIPA} {ASC})},
	pages = {2182--2188},
	booktitle = {2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference ({APSIPA} {ASC})},
	publisher = {{IEEE}},
	author = {Chaiwongyen, Anuwat and Duangpummet, Suradej and Karnjana, Jessada and Kongprawechnon, Waree and Unoki, Masashi},
	urldate = {2024-05-28},
	date = {2023-10-31},
	langid = {english},
	file = {Chaiwongyen et al. - 2023 - Deepfake-speech Detection with Pathological Featur.pdf:/home/jacob/Zotero/storage/JPYE4FYL/Chaiwongyen et al. - 2023 - Deepfake-speech Detection with Pathological Featur.pdf:application/pdf},
}

@inproceedings{li_comparative_2022,
	location = {Lisboa Portugal},
	title = {A Comparative Study on Physical and Perceptual Features for Deepfake Audio Detection},
	isbn = {978-1-4503-9496-3},
	url = {https://dl.acm.org/doi/10.1145/3552466.3556523},
	doi = {10.1145/3552466.3556523},
	abstract = {Audio content synthesis has stepped into a new era and brought a great threat to daily life since the development of deep learning techniques. The {ASVSpoof} Challenge and the {ADD} Challenge have been launched to motivate the development of Deepfake audio detection algorithms. Currently, the detection models, which consist of front-end feature extractors and back-end classifiers, utilize the physical features mainly, rather than the perceptual features that relate to natural emotions or breathiness. Therefore, we provide a comprehensive study on 16 physical and perceptual features and evaluate their effectiveness in both Track 1 and Track 2 of the {ADD} Challenge. Based on results, {PLP}, as a perceptual feature, outperforms the rest of the features in Track 1, while {CQCC} has the best performance in Track 2. Our experiments demonstrate the significance of perceptual features in detecting Deepfake audios. We also seek to explore the underlying characteristics of features that can distinguish Deepfake audio from a real one. We perform statistical analysis on each feature to show its distribution differences on real and synthesized audios. This paper will provide a potential direction in selecting appropriate feature extraction methods for the future implementation of detection models.},
	eventtitle = {{MM} '22: The 30th {ACM} International Conference on Multimedia},
	pages = {35--41},
	booktitle = {Proceedings of the 1st International Workshop on Deepfake Detection for Audio Multimedia},
	publisher = {{ACM}},
	author = {Li, Menglu and Ahmadiadli, Yasaman and Zhang, Xiao-Ping},
	urldate = {2024-05-11},
	date = {2022-10-14},
	langid = {english},
	file = {Li et al. - 2022 - A Comparative Study on Physical and Perceptual Fea.pdf:/home/jacob/Zotero/storage/RPQU57UG/Li et al. - 2022 - A Comparative Study on Physical and Perceptual Fea.pdf:application/pdf},
}

@inproceedings{qais_deepfake_2022,
	location = {Hyderabad, India},
	title = {Deepfake Audio Detection with Neural Networks Using Audio Features},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66547-258-6},
	url = {https://ieeexplore.ieee.org/document/9862519/},
	doi = {10.1109/ICICCSP53532.2022.9862519},
	abstract = {In this paper, a speech spoofing detection system based on Convolutional neural networks using different audio features has been proposed to classify the human speech and synthetic voice, Worst-case scenarios can develop using deepfake audios as threat to assets and image of a person, it can also become a threat to the whole country by unethical uses intended for loss of other party. Using a small voice clip of a person an attacker can develop similar voices. Every audio signal can be represented on a 2D graph plotted by mathematical calculations. The processing of audios into {CNN} requires a lot of computation, to make a system that can detect deepfake voices with much less computation by conversion of audios to images of audio features (Spectrogram, {MFCC}, {FFT}, {STFT} ) and then obtaining the array values as a numeric format which are most suitable to feed. Different approaches for feeding data to model are applied for prediction individually as well as in a concatenated approach.},
	eventtitle = {2022 International Conference on Intelligent Controller and Computing for Smart Power ({ICICCSP})},
	pages = {1--6},
	booktitle = {2022 International Conference on Intelligent Controller and Computing for Smart Power ({ICICCSP})},
	publisher = {{IEEE}},
	author = {Qais, Abu and Rastogi, Akshar and Saxena, Akash and Rana, Arpit and Sinha, Deependra},
	urldate = {2024-05-28},
	date = {2022-07-21},
	langid = {english},
	file = {Qais et al. - 2022 - Deepfake Audio Detection with Neural Networks Usin.pdf:/home/jacob/Zotero/storage/U4Z3ZRM3/Qais et al. - 2022 - Deepfake Audio Detection with Neural Networks Usin.pdf:application/pdf},
}

@inproceedings{xue_audio_2022,
	location = {Lisboa Portugal},
	title = {Audio Deepfake Detection Based on a Combination of F0 Information and Real Plus Imaginary Spectrogram Features},
	isbn = {978-1-4503-9496-3},
	url = {https://dl.acm.org/doi/10.1145/3552466.3556526},
	doi = {10.1145/3552466.3556526},
	eventtitle = {{MM} '22: The 30th {ACM} International Conference on Multimedia},
	pages = {19--26},
	booktitle = {Proceedings of the 1st International Workshop on Deepfake Detection for Audio Multimedia},
	publisher = {{ACM}},
	author = {Xue, Jun and Fan, Cunhang and Lv, Zhao and Tao, Jianhua and Yi, Jiangyan and Zheng, Chengshi and Wen, Zhengqi and Yuan, Minmin and Shao, Shegang},
	urldate = {2024-05-11},
	date = {2022-10-14},
	langid = {english},
	file = {Xue et al. - 2022 - Audio Deepfake Detection Based on a Combination of.pdf:/home/jacob/Zotero/storage/H7BIS2GL/Xue et al. - 2022 - Audio Deepfake Detection Based on a Combination of.pdf:application/pdf},
}

@inproceedings{khanjani_learning_2023,
	location = {Charlotte, {NC}, {USA}},
	title = {Learning to Listen and Listening to Learn: Spoofed Audio Detection Through Linguistic Data Augmentation},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {9798350337730},
	url = {https://ieeexplore.ieee.org/document/10297267/},
	doi = {10.1109/ISI58743.2023.10297267},
	shorttitle = {Learning to Listen and Listening to Learn},
	abstract = {Spoofed audio, both human or machine generated, causes deception and disinformation and as such is a societal challenge. This study advances the detection of spoofed audio through a novel approach that augments knowledge about audio data by incorporating linguistic information. Using perceptual methods, for English audio samples, experts in sociolinguistics listened for audio cues, and used binary labels to indicate the perceived authenticity of a set of speech samples, based on phonetic and phonological features that occur frequently in spoken English. These Expert Defined Linguistic Features ({EDLFs}) were then used in supervised spoofed audio detection methods to augment {AI} models. An ensemble method based on multi-domain features both from the audio data itself and the {EDLFs} was also created to evaluate the spoofed audio detection, and to demonstrate how {EDLFs} can improve traditional methods of spoofed audio detection. We found that augmenting the audio data with expertinformed linguistic annotation increased the accuracy of spoofed audio detection significantly in both the training and testing datasets across the evaluated single and ensemble models. Our findings indicate the promising avenue of augmenting audio data with perceptual linguistic techniques, as a method of human discernment, to enhance {AI}-based approaches for spoofed audio detection. These features also establish a foundation for direct linguistic annotations on new audio clips for robust spoofed audio detection.},
	eventtitle = {2023 {IEEE} International Conference on Intelligence and Security Informatics ({ISI})},
	pages = {01--06},
	booktitle = {2023 {IEEE} International Conference on Intelligence and Security Informatics ({ISI})},
	publisher = {{IEEE}},
	author = {Khanjani, Zahra and Davis, Lavon and Tuz, Anna and Nwosu, Kifekachukwu and Mallinson, Christine and Janeja, Vandana P.},
	urldate = {2024-05-28},
	date = {2023-10-02},
	langid = {english},
	file = {Khanjani et al. - 2023 - Learning to Listen and Listening to Learn Spoofed.pdf:/home/jacob/Zotero/storage/MDJK3BG3/Khanjani et al. - 2023 - Learning to Listen and Listening to Learn Spoofed.pdf:application/pdf},
}

@misc{muller_does_2022,
	title = {Does Audio Deepfake Detection Generalize?},
	url = {http://arxiv.org/abs/2203.16263},
	abstract = {Current text-to-speech algorithms produce realistic fakes of human voices, making deepfake detection a much-needed area of research. While researchers have presented various deep learning models for audio spoofs detection, it is often unclear exactly why these architectures are successful: Preprocessing steps, hyperparameter settings, and the degree of ﬁne-tuning are not consistent across related work. Which factors contribute to success, and which are accidental? In this work, we address this problem: We systematize audio spooﬁng detection by re-implementing and uniformly evaluating twelve architectures from related work. We identify overarching features for successful audio deepfake detection, such as using cqtspec or logspec features instead of melspec features, which improves performance by 37\% {EER} on average, all other factors constant.},
	number = {{arXiv}:2203.16263},
	publisher = {{arXiv}},
	author = {Müller, Nicolas M. and Czempin, Pavel and Dieckmann, Franziska and Froghyar, Adam and Böttinger, Konstantin},
	urldate = {2024-07-04},
	date = {2022-04-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2203.16263 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {Müller et al. - 2022 - Does Audio Deepfake Detection Generalize.pdf:/home/jacob/Zotero/storage/YBAF5V2M/Müller et al. - 2022 - Does Audio Deepfake Detection Generalize.pdf:application/pdf},
}

@misc{barrington_single_2023,
	title = {Single and Multi-Speaker Cloned Voice Detection: From Perceptual to Learned Features},
	url = {http://arxiv.org/abs/2307.07683},
	doi = {10.48550/arXiv.2307.07683},
	shorttitle = {Single and Multi-Speaker Cloned Voice Detection},
	abstract = {Synthetic-voice cloning technologies have seen significant advances in recent years, giving rise to a range of potential harms. From small- and large-scale financial fraud to disinformation campaigns, the need for reliable methods to differentiate real and synthesized voices is imperative. We describe three techniques for differentiating a real from a cloned voice designed to impersonate a specific person. These three approaches differ in their feature extraction stage with lowdimensional perceptual features offering high interpretability but lower accuracy, to generic spectral features, and end-to-end learned features offering less interpretability but higher accuracy. We show the efficacy of these approaches when trained on a single speaker’s voice and when trained on multiple voices. The learned features consistently yield an equal error rate between 0\% and 4\%, and are reasonably robust to adversarial laundering.},
	number = {{arXiv}:2307.07683},
	publisher = {{arXiv}},
	author = {Barrington, Sarah and Barua, Romit and Koorma, Gautham and Farid, Hany},
	urldate = {2024-12-18},
	date = {2023-09-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2307.07683 [cs]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language},
	file = {PDF:/home/jacob/Zotero/storage/EIMG2L3I/Barrington et al. - 2023 - Single and Multi-Speaker Cloned Voice Detection From Perceptual to Learned Features.pdf:application/pdf},
}

@misc{ge_explaining_2024,
	title = {Explaining deep learning models for spoofing and deepfake detection with {SHapley} Additive {exPlanations}},
	url = {http://arxiv.org/abs/2110.03309},
	doi = {10.48550/arXiv.2110.03309},
	abstract = {Substantial progress in spoofing and deepfake detection has been made in recent years. Nonetheless, the community has yet to make notable inroads in providing an explanation for how a classifier produces its output. The dominance of black box spoofing detection solutions is at further odds with the drive toward trustworthy, explainable artificial intelligence. This paper describes our use of {SHapley} Additive {exPlanations} ({SHAP}) to gain new insights in spoofing detection. We demonstrate use of the tool in revealing unexpected classifier behaviour, the artefacts that contribute most to classifier outputs and differences in the behaviour of competing spoofing detection models. The tool is both efficient and flexible, being readily applicable to a host of different architecture models in addition to related, different applications. All results reported in the paper are reproducible using open-source software.},
	number = {{arXiv}:2110.03309},
	publisher = {{arXiv}},
	author = {Ge, Wanying and Patino, Jose and Todisco, Massimiliano and Evans, Nicholas},
	urldate = {2024-12-18},
	date = {2024-04-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2110.03309 [eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {PDF:/home/jacob/Zotero/storage/9R26CPNM/Ge et al. - 2024 - Explaining deep learning models for spoofing and deepfake detection with SHapley Additive exPlanatio.pdf:application/pdf},
}

@article{haq_multimodal_2023,
	title = {Multimodal Neurosymbolic Approach for Explainable Deepfake Detection},
	issn = {1551-6857, 1551-6865},
	url = {https://dl.acm.org/doi/10.1145/3624748},
	doi = {10.1145/3624748},
	abstract = {Deepfake detection has become increasingly important in recent years owing to the widespread availability of deepfake generation technologies. Existing deepfake detection methods present two primary limitations i.e., trained on a specific type of deepfake dataset, which renders them vulnerable to unseen deepfakes; and they regard deepfakes as a black-box with limited explainability, making it difficult for non-{AI} experts to understand and trust the decisions. Hence, this paper proposes a novel neurosymbolic deepfake detection framework that exploits the fact that human emotions cannot be imitated easily owing to their complex nature. We argue that deep fakes typically exhibit inter- or intra- modality inconsistencies in the emotional expressions of the person being manipulated. Thus, the proposed framework performs inter- and intra- modality reasoning on emotions extracted from audio and visual modalities using a psychological and arousalvalence model for deepfake detection. In addition to fake detection, the proposed framework provides textual explanations for its decisions. The results obtained using Presidential Deepfakes Dataset and World Leaders Dataset of real and manipulated videos demonstrate the effectiveness of our approach in detecting deepfakes and highlight the potential of neurosymbolic approach for expandability.},
	pages = {3624748},
	journaltitle = {{ACM} Transactions on Multimedia Computing, Communications, and Applications},
	shortjournal = {{ACM} Trans. Multimedia Comput. Commun. Appl.},
	author = {Haq, Ijaz Ul and Malik, Khalid Mahmood and Muhammad, Khan},
	urldate = {2024-05-11},
	date = {2023-09-20},
	langid = {english},
	file = {Haq et al. - 2023 - Multimodal Neurosymbolic Approach for Explainable .pdf:/home/jacob/Zotero/storage/2XAC7A5W/Haq et al. - 2023 - Multimodal Neurosymbolic Approach for Explainable .pdf:application/pdf},
}

@inproceedings{cuccovillo_open_2022,
	title = {Open Challenges in Synthetic Speech Detection},
	url = {http://arxiv.org/abs/2209.07180},
	doi = {10.1109/WIFS55849.2022.9975433},
	abstract = {In this paper the current status and open challenges of synthetic speech detection are addressed. The work comprises an initial analysis of available open datasets and of existing detection methods, a description of the requirements for new research datasets compliant with regulations and better representing real-case scenarios, and a discussion of the desired characteristics of future trustworthy detection methods in terms of both functional and non-functional requirements. Compared to other works, based on speciﬁc detection solutions or presenting single dataset of synthetic speeches, our paper is meant to orient future state-of-the-art research in the domain, to quickly lessen the current gap between synthesis and detection approaches.},
	pages = {1--6},
	booktitle = {2022 {IEEE} International Workshop on Information Forensics and Security ({WIFS})},
	author = {Cuccovillo, Luca and Papastergiopoulos, Christoforos and Vafeiadis, Anastasios and Yaroshchuk, Artem and Aichroth, Patrick and Votis, Konstantinos and Tzovaras, Dimitrios},
	urldate = {2024-12-18},
	date = {2022-12-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2209.07180 [eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {PDF:/home/jacob/Zotero/storage/93FTPZLY/Cuccovillo et al. - 2022 - Open Challenges in Synthetic Speech Detection.pdf:application/pdf},
}

@article{goodman_european_2017,
	title = {European Union regulations on algorithmic decision-making and a "right to explanation"},
	volume = {38},
	issn = {0738-4602, 2371-9621},
	url = {http://arxiv.org/abs/1606.08813},
	doi = {10.1609/aimag.v38i3.2741},
	abstract = {We summarize the potential impact that the European Union’s new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the {EU} in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on userlevel predictors) which “signiﬁcantly affect” users. The law will also effectively create a “right to explanation,” whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for computer scientists to take the lead in designing algorithms and evaluation frameworks which avoid discrimination and enable explanation.},
	pages = {50--57},
	number = {3},
	journaltitle = {{AI} Magazine},
	shortjournal = {{AI} Magazine},
	author = {Goodman, Bryce and Flaxman, Seth},
	urldate = {2025-01-18},
	date = {2017-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1606.08813 [stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computers and Society, Statistics - Machine Learning},
	file = {PDF:/home/jacob/Zotero/storage/92FH2PJU/Goodman and Flaxman - 2017 - European Union regulations on algorithmic decision-making and a right to explanation.pdf:application/pdf},
}

@article{sankaranarayanan_presidential_nodate,
	title = {The Presidential Deepfakes Dataset},
	abstract = {How do we evaluate media forensic techniques for detecting deepfakes? We present the Presidential Deepfakes Dataset ({PDD}), which consists of 32 videos, half of which are original videos and half of which are manipulated with audio impersonations, synthesized lip synchronizations, political misinformation, and situational artifacts. This dataset expands the context on which end-to-end media forensic systems can be evaluated. As an example, we evaluate the winning model of the {DeepFake} Detection Challenge on the {PDD} and find that it classifies 69\% of the videos in the {PDD} accurately. We share this dataset publicly for researchers to evaluate their techniques with the intention of pre-bunking future misinformation attempts.},
	author = {Sankaranarayanan, Aruna and Groh, Matthew and Picard, Rosalind and Lippman, Andrew},
	langid = {english},
	file = {PDF:/home/jacob/Zotero/storage/BRLMG5C3/Sankaranarayanan et al. - The Presidential Deepfakes Dataset.pdf:application/pdf},
}

@misc{wang_asvspoof_2020,
	title = {{ASVspoof} 2019: A large-scale public database of synthesized, converted and replayed speech},
	url = {http://arxiv.org/abs/1911.01601},
	shorttitle = {{ASVspoof} 2019},
	abstract = {Automatic speaker veriﬁcation ({ASV}) is one of the most natural and convenient means of biometric person recognition. Unfortunately, just like all other biometric systems, {ASV} is vulnerable to spooﬁng, also referred to as “presentation attacks.” These vulnerabilities are generally unacceptable and call for spooﬁng countermeasures or “presentation attack detection” systems. In addition to impersonation, {ASV} systems are vulnerable to replay, speech synthesis, and voice conversion attacks.},
	number = {{arXiv}:1911.01601},
	publisher = {{arXiv}},
	author = {Wang, Xin and Yamagishi, Junichi and Todisco, Massimiliano and Delgado, Hector and Nautsch, Andreas and Evans, Nicholas and Sahidullah, Md and Vestman, Ville and Kinnunen, Tomi and Lee, Kong Aik and Juvela, Lauri and Alku, Paavo and Peng, Yu-Huai and Hwang, Hsin-Te and Tsao, Yu and Wang, Hsin-Min and Maguer, Sebastien Le and Becker, Markus and Henderson, Fergus and Clark, Rob and Zhang, Yu and Wang, Quan and Jia, Ye and Onuma, Kai and Mushika, Koji and Kaneda, Takashi and Jiang, Yuan and Liu, Li-Juan and Wu, Yi-Chiao and Huang, Wen-Chin and Toda, Tomoki and Tanaka, Kou and Kameoka, Hirokazu and Steiner, Ingmar and Matrouf, Driss and Bonastre, Jean-Francois and Govender, Avashna and Ronanki, Srikanth and Zhang, Jing-Xuan and Ling, Zhen-Hua},
	urldate = {2024-07-03},
	date = {2020-07-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1911.01601 [cs, eess]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing},
	file = {Wang et al. - 2020 - ASVspoof 2019 A large-scale public database of sy.pdf:/home/jacob/Zotero/storage/JVRQ5PHZ/Wang et al. - 2020 - ASVspoof 2019 A large-scale public database of sy.pdf:application/pdf},
}

@misc{yi_add_2024,
	title = {{ADD} 2022: the First Audio Deep Synthesis Detection Challenge},
	url = {http://arxiv.org/abs/2202.08433},
	doi = {10.48550/arXiv.2202.08433},
	shorttitle = {{ADD} 2022},
	abstract = {Audio deepfake detection is an emerging topic, which was included in the {ASVspoof} 2021. However, the recent shared tasks have not covered many real-life and challenging scenarios. The ﬁrst Audio Deep synthesis Detection challenge ({ADD}) was motivated to ﬁll in the gap. The {ADD} 2022 includes three tracks: low-quality fake audio detection ({LF}), partially fake audio detection ({PF}) and audio fake game ({FG}). The {LF} track focuses on dealing with bona ﬁde and fully fake utterances with various real-world noises etc. The {PF} track aims to distinguish the partially fake audio from the real. The {FG} track is a rivalry game, which includes two tasks: an audio generation task and an audio fake detection task. In this paper, we describe the datasets, evaluation metrics, and protocols. We also report major ﬁndings that reﬂect the recent advances in audio deepfake detection tasks. The {ADD} 2022 dataset is publicly available, see Train\&Dev1, Adaption2, Track1 eval3, Track2 eval4, Track3.2 R1 eval5, Track3.2 R2 eval6.},
	number = {{arXiv}:2202.08433},
	publisher = {{arXiv}},
	author = {Yi, Jiangyan and Fu, Ruibo and Tao, Jianhua and Nie, Shuai and Ma, Haoxin and Wang, Chenglong and Wang, Tao and Tian, Zhengkun and Zhang, Xiaohui and Bai, Ye and Fan, Cunhang and Liang, Shan and Wang, Shiming and Zhang, Shuai and Yan, Xinrui and Xu, Le and Wen, Zhengqi and Li, Haizhou and Lian, Zheng and Liu, Bin},
	urldate = {2025-01-22},
	date = {2024-07-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2202.08433 [cs]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {PDF:/home/jacob/Zotero/storage/XI7CX9R9/Yi et al. - 2024 - ADD 2022 the First Audio Deep Synthesis Detection Challenge.pdf:application/pdf},
}

@misc{ribeiro_why_2016,
	title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
	url = {http://arxiv.org/abs/1602.04938},
	doi = {10.48550/arXiv.1602.04938},
	shorttitle = {"Why Should I Trust You?},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose {LIME}, a novel explanation technique that explains the predictions of any classiﬁer in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the ﬂexibility of these methods by explaining diﬀerent models for text (e.g. random forests) and image classiﬁcation (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classiﬁer, and identifying why a classiﬁer should not be trusted.},
	number = {{arXiv}:1602.04938},
	publisher = {{arXiv}},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	urldate = {2025-01-18},
	date = {2016-08-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1602.04938 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {PDF:/home/jacob/Zotero/storage/KGW5D3MP/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predictions of Any Classifier.pdf:application/pdf},
}

@inproceedings{yang_robust_2024,
	location = {Seoul, Korea, Republic of},
	title = {A Robust Audio Deepfake Detection System via Multi-View Feature},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-4485-1},
	url = {https://ieeexplore.ieee.org/document/10446560/},
	doi = {10.1109/ICASSP48485.2024.10446560},
	abstract = {With the advancement of generative modeling techniques, synthetic human speech becomes increasingly indistinguishable from real, and tricky challenges are elicited for the audio deepfake detection ({ADD}) system. In this paper, we exploit audio features to improve the generalizability of {ADD} systems. Investigation of the {ADD} task performance is conducted over a broad range of audio features, including various handcrafted features and learning-based features. Experiments show that learning-based audio features pretrained on a large amount of data generalize better than hand-crafted features on out-of-domain scenarios. Subsequently, we further improve the generalizability of the {ADD} system using proposed multi-feature approaches to incorporate complimentary information from features of different views. The model trained on {ASV}2019 data achieves an equal error rate of 24.27\% on the In-the-Wild dataset. The code will be released as soon 1.},
	eventtitle = {{ICASSP} 2024 - 2024 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {13131--13135},
	booktitle = {{ICASSP} 2024 - 2024 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	publisher = {{IEEE}},
	author = {Yang, Yujie and Qin, Haochen and Zhou, Hang and Wang, Chengcheng and Guo, Tianyu and Han, Kai and Wang, Yunhe},
	urldate = {2024-05-28},
	date = {2024-04-14},
	langid = {english},
	file = {Yang et al. - 2024 - A Robust Audio Deepfake Detection System via Multi.pdf:/home/jacob/Zotero/storage/W9YJ42T9/Yang et al. - 2024 - A Robust Audio Deepfake Detection System via Multi.pdf:application/pdf},
}

@inproceedings{ranjan_statnet_2022,
	location = {Abu Dhabi, United Arab Emirates},
	title = {{STATNet}: Spectral and Temporal features based Multi-Task Network for Audio Spoofing Detection},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-6654-6394-2},
	url = {https://ieeexplore.ieee.org/document/10007949/},
	doi = {10.1109/IJCB54206.2022.10007949},
	shorttitle = {{STATNet}},
	abstract = {With the rise in mobile phone users and {VoIP}, voice has emerged as an easy and accessible biometric modality for identiﬁcation or veriﬁcation tasks. Given the increasing usage of voice biometrics, the security of these systems is also of paramount importance. Researchers have demonstrated that Automatic Speaker Veriﬁcation ({ASV}) systems are prone to spooﬁng attacks like synthetic speech or fake speech, which can be used maliciously for a variety of tasks such as impersonation, fake news spreading, and opinion formation. This research proposes a deep convolutionbased multi-task network which performs both spoof detection and source identiﬁcation for synthetic speech. The proposed model is evaluated on three datasets {ASVspoof}2019 {LA}, {FOR}-Norm and In-the-Wild Audio Deepfake dataset. The results demonstrate the {EER} of 2.456\%, 0.814\%, and 0.199\% on the {ASVspoof}2019 {LA}, {FOR}-Norm, and Inthe-Wild Audio Deepfake datasets. In addition, we have also demonstrated results for cross-dataset evaluation and speech source identiﬁcation.},
	eventtitle = {2022 {IEEE} International Joint Conference on Biometrics ({IJCB})},
	pages = {1--9},
	booktitle = {2022 {IEEE} International Joint Conference on Biometrics ({IJCB})},
	publisher = {{IEEE}},
	author = {Ranjan, Rishabh and Vatsa, Mayank and Singh, Richa},
	urldate = {2024-05-28},
	date = {2022-10-10},
	langid = {english},
	file = {Ranjan et al. - 2022 - STATNet Spectral and Temporal features based Mult.pdf:/home/jacob/Zotero/storage/VXRIW4Q4/Ranjan et al. - 2022 - STATNet Spectral and Temporal features based Mult.pdf:application/pdf},
}

@inproceedings{sharevski_blind_2024,
	location = {Salt Lake City {UT} {USA}},
	title = {Blind and Low-Vision Individuals' Detection of Audio Deepfakes},
	isbn = {979-8-4007-0636-3},
	url = {https://dl.acm.org/doi/10.1145/3658644.3690305},
	doi = {10.1145/3658644.3690305},
	abstract = {Audio deepfakes are a form of deception where convincing speech sentences are synthesized through machine learning means to give an impression of a human speaker. Audio deepfakes emerge as an attractive vector for targeting users that rely on audio accessibility, such as individuals who are blind or low vision. The critical reliance on speech both as a medium and an affordance puts this population at an undue risk of being deceived as they rely solely on themselves to detect whether a piece of audio is a deepfake or not. To better understand the nature of this risk considering the nuanced reliance on assistive technologies such as screen readers, we conducted a user study with n=16 blind and low vision individuals from the {US}. Our participants achieved an overall discernment accuracy of 59\%, and clips identified as deep fakes were only actually deepfakes in 50.8\% of the cases (precision). The participants that self-identified as “low vision” performed slightly better (accuracy of 61\%, precision of 64\%) compared to the ones that self-identified as “blind” (accuracy of 55\%, precision of 56\%). Our qualitative results show that the participants in the “blind” group mostly considered a combination of infliction, imperfections in the voice, and the intensity in the speech delivery as discernment factors. The participants in the “low vision” group mostly used the speaker’s pitch, enunciation, emotion, and the fluency and articulation of the speaker as discernment cues. Overall, participants felt that audio deepfakes have the potential to deceive visually impaired individuals with political disinformation, impersonate their voice in authentication and smart homes, and specifically target them with voice phishing and enhanced scams.},
	eventtitle = {{CCS} '24: {ACM} {SIGSAC} Conference on Computer and Communications Security},
	pages = {4867--4881},
	booktitle = {Proceedings of the 2024 on {ACM} {SIGSAC} Conference on Computer and Communications Security},
	publisher = {{ACM}},
	author = {Sharevski, Filipo and Zeidieh, Aziz and Loop, Jennifer Vander and Jachim, Peter},
	urldate = {2024-12-17},
	date = {2024-12-02},
	langid = {english},
	file = {PDF:/home/jacob/Zotero/storage/E88FGDM8/Sharevski et al. - 2024 - Blind and Low-Vision Individuals' Detection of Audio Deepfakes.pdf:application/pdf},
}

@inproceedings{liu_hidden--wave_2023,
	location = {Florence, Italy},
	title = {Hidden-in-Wave: A Novel Idea to Camouflage {AI}-Synthesized Voices Based on Speaker-Irrelative Features},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {9798350315943},
	url = {https://ieeexplore.ieee.org/document/10301243/},
	doi = {10.1109/ISSRE59848.2023.00029},
	shorttitle = {Hidden-in-Wave},
	abstract = {Voice is an essential medium for human communication and collaboration, and its trustworthiness is of great importance to humans. Synthesizing fake voices and detecting synthesized voices are two sides of a coin. Both sides have made great strides with the recently prospering deep learning techniques. Attackers started using {AI} techniques to synthesize, even clone, human voices. Researchers also proposed a series of {AIsynthesized} voice detection approaches and achieved promising results in laboratory environments.},
	eventtitle = {2023 {IEEE} 34th International Symposium on Software Reliability Engineering ({ISSRE})},
	pages = {786--794},
	booktitle = {2023 {IEEE} 34th International Symposium on Software Reliability Engineering ({ISSRE})},
	publisher = {{IEEE}},
	author = {Liu, Xin and Tan, Yuan and Hai, Xuan and Yu, Qingchen and Zhou, Qingguo},
	urldate = {2024-05-28},
	date = {2023-10-09},
	langid = {english},
	file = {Liu et al. - 2023 - Hidden-in-Wave A Novel Idea to Camouflage AI-Synt.pdf:/home/jacob/Zotero/storage/URMFNINV/Liu et al. - 2023 - Hidden-in-Wave A Novel Idea to Camouflage AI-Synt.pdf:application/pdf},
}

@inproceedings{anagha_audio_2023,
	location = {Moradabad, India},
	title = {Audio Deepfake Detection Using Deep Learning},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {9798350369861 9798350369885},
	url = {https://ieeexplore.ieee.org/document/10428163/},
	doi = {10.1109/SMART59791.2023.10428163},
	abstract = {The capacity to identify real audio recordings from their modified counterparts is essential in the age of sophisticated digital manipulation for maintaining security and trust in a vari- ety of applications, from media forensics to voice authentication systems. This research aims to create a deep learning model that can distinguish between authentic and altered audio files, with an emphasis on identifying audio deepfakes. The study uses Mel spectrogram representations and data augmentation techniques to effectively extract features from the {ASVspoof} 2019 dataset and train models. Convolutional neural networks ({CNNs}) comprising a number of layers, including convolutional, pooling, batch normalization, {ReLU} activation, dropout, global average pooling, and a dense classification layer are used as the foundation of the design. The Adam optimizer is used to optimize the model once it has been trained using binary cross-entropy loss, and a variety of metrics, such as accuracy, F1 score, {ROC} curve, and {AUC}, are used to track its performance. By making it easier to identify audio deepfakes, this project will ultimately increase the security and integrity of audio data in the digital world.},
	eventtitle = {2023 12th International Conference on System Modeling \& Advancement in Research Trends ({SMART})},
	pages = {176--181},
	booktitle = {2023 12th International Conference on System Modeling \& Advancement in Research Trends ({SMART})},
	publisher = {{IEEE}},
	author = {Anagha, R. and Arya, A. and Narayan, V. Hari and Abhishek, S. and Anjali, T.},
	urldate = {2024-05-28},
	date = {2023-12-22},
	langid = {english},
	file = {Anagha et al. - 2023 - Audio Deepfake Detection Using Deep Learning.pdf:/home/jacob/Zotero/storage/V9CKUJXV/Anagha et al. - 2023 - Audio Deepfake Detection Using Deep Learning.pdf:application/pdf},
}

@inproceedings{yan_initial_2022,
	location = {Lisboa Portugal},
	title = {An Initial Investigation for Detecting Vocoder Fingerprints of Fake Audio},
	isbn = {978-1-4503-9496-3},
	url = {https://dl.acm.org/doi/10.1145/3552466.3556525},
	doi = {10.1145/3552466.3556525},
	abstract = {Many effective attempts have been made for fake audio detection. However, they can only provide detection results but no countermeasures to curb this harm. For many related practical applications, what model or algorithm generated the fake audio also is needed. Therefore, We propose a new problem for detecting vocoder fingerprints of fake audio. Experiments are conducted on the datasets synthesized by eight state-of-the-art vocoders. We have preliminarily explored the features and model architectures. The t-{SNE} visualization shows that different vocoders generate distinct vocoder fingerprints.},
	eventtitle = {{MM} '22: The 30th {ACM} International Conference on Multimedia},
	pages = {61--68},
	booktitle = {Proceedings of the 1st International Workshop on Deepfake Detection for Audio Multimedia},
	publisher = {{ACM}},
	author = {Yan, Xinrui and Yi, Jiangyan and Tao, Jianhua and Wang, Chenglong and Ma, Haoxin and Wang, Tao and Wang, Shiming and Fu, Ruibo},
	urldate = {2024-05-11},
	date = {2022-10-14},
	langid = {english},
	file = {Yan et al. - 2022 - An Initial Investigation for Detecting Vocoder Fin.pdf:/home/jacob/Zotero/storage/7369Q38A/Yan et al. - 2022 - An Initial Investigation for Detecting Vocoder Fin.pdf:application/pdf},
}

@inproceedings{delgado_asvspoof_2018,
	title = {{ASVspoof} 2017 Version 2.0: meta-data analysis and baseline enhancements},
	url = {https://www.isca-archive.org/odyssey_2018/delgado18_odyssey.html},
	doi = {10.21437/Odyssey.2018-42},
	shorttitle = {{ASVspoof} 2017 Version 2.0},
	abstract = {The now-acknowledged vulnerabilities of automatic speaker veriﬁcation ({ASV}) technology to spooﬁng attacks have spawned interests to develop so-called spooﬁng countermeasures. By providing common databases, protocols and metrics for their assessment, the {ASVspoof} initiative was born to spearhead research in this area. The ﬁrst competitive {ASVspoof} challenge held in 2015 focused on the assessment of countermeasures to protect {ASV} technology from voice conversion and speech synthesis spooﬁng attacks. The second challenge switched focus to the consideration of replay spooﬁng attacks and countermeasures. This paper describes Version 2.0 of the {ASVspoof} 2017 database which was released to correct data anomalies detected post-evaluation. The paper contains as-yet unpublished meta-data which describes recording and playback devices and acoustic environments. These support the analysis of replay detection performance and limits. Also described are new results for the ofﬁcial {ASVspoof} baseline system which is based upon a constant Q cesptral coefﬁcient frontend and a Gaussian mixture model backend. Reported are enhancements to the baseline system in the form of log-energy coefﬁcients and cepstral mean and variance normalisation in addition to an alternative i-vector backend. The best results correspond to a 48\% relative reduction in equal error rate when compared to the original baseline system.},
	eventtitle = {The Speaker and Language Recognition Workshop (Odyssey 2018)},
	pages = {296--303},
	booktitle = {The Speaker and Language Recognition Workshop (Odyssey 2018)},
	publisher = {{ISCA}},
	author = {Delgado, Héctor and Todisco, Massimiliano and Sahidullah, Md and Evans, Nicholas and Kinnunen, Tomi and Lee, Kong Aik and Yamagishi, Junichi},
	urldate = {2025-02-07},
	date = {2018-06-26},
	langid = {english},
	file = {PDF:/home/jacob/Zotero/storage/4H6MBJSX/Delgado et al. - 2018 - ASVspoof 2017 Version 2.0 meta-data analysis and baseline enhancements.pdf:application/pdf},
}

@inproceedings{warren_better_2024,
	location = {Salt Lake City {UT} {USA}},
	title = {"Better Be Computer or I'm Dumb": A Large-Scale Evaluation of Humans as Audio Deepfake Detectors},
	isbn = {979-8-4007-0636-3},
	url = {https://dl.acm.org/doi/10.1145/3658644.3670325},
	doi = {10.1145/3658644.3670325},
	shorttitle = {"Better Be Computer or I'm Dumb"},
	abstract = {Audio deepfakes represent a rising threat to trust in our daily communications. In response to this, the research community has developed a wide array of detection techniques aimed at preventing such attacks from deceiving users. Unfortunately, the creation of these defenses has generally overlooked the most important element of the system - the user themselves. As such, it is not clear whether current mechanisms augment, hinder, or simply contradict human classification of deepfakes. In this paper, we perform the first large-scale user study on deepfake detection. We recruit over 1,200 users and present them with samples from the three most widely-cited deepfake datasets. We then quantitatively compare performance and qualitatively conduct thematic analysis to motivate and understand the reasoning behind user decisions and differences from machine classifications. Our results show that users correctly classify human audio at significantly higher rates than machine learning models, and rely on linguistic features and intuition when performing classification. However, users are also regularly misled by pre-conceptions about the capabilities of generated audio (e.g., that accents and background sounds are indicative of humans). Finally, machine learning models suffer from significantly higher ∗The title comes from the free response portion of our study. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.},
	eventtitle = {{CCS} '24: {ACM} {SIGSAC} Conference on Computer and Communications Security},
	pages = {2696--2710},
	booktitle = {Proceedings of the 2024 on {ACM} {SIGSAC} Conference on Computer and Communications Security},
	publisher = {{ACM}},
	author = {Warren, Kevin and Tucker, Tyler and Crowder, Anna and Olszewski, Daniel and Lu, Allison and Fedele, Caroline and Pasternak, Magdalena and Layton, Seth and Butler, Kevin and Gates, Carrie and Traynor, Patrick},
	urldate = {2025-02-13},
	date = {2024-12-02},
	langid = {english},
	file = {PDF:/home/jacob/Zotero/storage/JVHDXXYF/Warren et al. - 2024 - Better Be Computer or I'm Dumb A Large-Scale Evaluation of Humans as Audio Deepfake Detectors.pdf:application/pdf},
}

@inproceedings{fathan_mel-spectrogram_2022,
	location = {Taipei, Taiwan},
	title = {Mel-Spectrogram Image-Based End-to-End Audio Deepfake Detection Under Channel-Mismatched Conditions},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66548-563-0},
	url = {https://ieeexplore.ieee.org/document/9859621/},
	doi = {10.1109/ICME52920.2022.9859621},
	abstract = {This work focuses on the problem of detecting fake audio clips. To improve current audio spoofing detection models, we propose a selection of multiple audio augmentations specially designed to resemble audio spoofing attacks. These augmentations are experimentally found to be very useful and using them achieves a notable performance of 2.8\% {EER} on the {ASVspoof} 2019 challenge evaluation set. Unlike the widely employed acoustic features, in this paper we explore the use of Mel-spectrogram image features and employ various audio codecs to achieve robustness to codec and transmission channel variability present in the {ASVspoof}2021 Evaluation set. To better handle spectral information, crucial to detect spoofing, we adopt the {WaveletCNN} and {VGG}16 architectures which outperform all baselines. Finally, we find that robustness of countermeasure systems degrades dramatically when provided with speech samples degraded through {VoIP} network transmission or mismatching audio compression.},
	eventtitle = {2022 {IEEE} International Conference on Multimedia and Expo ({ICME})},
	pages = {1--6},
	booktitle = {2022 {IEEE} International Conference on Multimedia and Expo ({ICME})},
	publisher = {{IEEE}},
	author = {Fathan, Abderrahim and Alam, Jahangir and Kang, Woo Hyun},
	urldate = {2024-05-28},
	date = {2022-07-18},
	langid = {english},
	file = {Fathan et al. - 2022 - Mel-Spectrogram Image-Based End-to-End Audio Deepf.pdf:/home/jacob/Zotero/storage/MMZHDTXI/Fathan et al. - 2022 - Mel-Spectrogram Image-Based End-to-End Audio Deepf.pdf:application/pdf},
}

@inproceedings{altalahin_unmasking_2023,
	location = {Amman, Jordan},
	title = {Unmasking the Truth: A Deep Learning Approach to Detecting Deepfake Audio Through {MFCC} Features},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {9798350320060},
	url = {https://ieeexplore.ieee.org/document/10226172/},
	doi = {10.1109/ICIT58056.2023.10226172},
	shorttitle = {Unmasking the Truth},
	abstract = {Deepfake content is artificially created or altered using artificial intelligence ({AI}) methods to appear real. Synthesis can include audio, video, images, and text. Deepfakes may now produce content that looks normal, making it more difficult to identify. Significant progress has been made in identifying video deep fakes in recent years; However, most of the investigations into voice deep fake detection have used the {ASVSpoof}-2019 dataset and several machine learning and deep learning algorithms. This research uses machine-based and deep-learning approaches to identify fake audio. Melted frequency cepstral coefficients ({MFCCs}) are used to extract the most useful information from the sound. We choose the 2019 {ASVSpoof} dataset, which is the latest reference dataset. Experimental results show that Convolutional Neural Networks ({CNN}): ({CNN}-{LSTM}) outperformed other machine learning ({ML}) models in terms of accuracy, achieving an accuracy of up to 88\%.},
	eventtitle = {2023 International Conference on Information Technology ({ICIT})},
	pages = {511--518},
	booktitle = {2023 International Conference on Information Technology ({ICIT})},
	publisher = {{IEEE}},
	author = {Altalahin, Islam and {AlZu}'bi, Shadi and Alqudah, Assal and Mughaid, Ala},
	urldate = {2024-05-28},
	date = {2023-08-09},
	langid = {english},
	file = {Altalahin et al. - 2023 - Unmasking the Truth A Deep Learning Approach to D.pdf:/home/jacob/Zotero/storage/BVLU8YEH/Altalahin et al. - 2023 - Unmasking the Truth A Deep Learning Approach to D.pdf:application/pdf},
}

@article{hamza_deepfake_2022,
	title = {Deepfake Audio Detection via {MFCC} Features Using Machine Learning},
	volume = {10},
	rights = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9996362/},
	doi = {10.1109/ACCESS.2022.3231480},
	abstract = {Deepfake content is created or altered synthetically using artiﬁcial intelligence ({AI}) approaches to appear real. It can include synthesizing audio, video, images, and text. Deepfakes may now produce natural-looking content, making them harder to identify. Much progress has been achieved in identifying video deepfakes in recent years; nevertheless, most investigations in detecting audio deepfakes have employed the {ASVSpoof} or {AVSpoof} dataset and various machine learning, deep learning, and deep learning algorithms. This research uses machine and deep learning-based approaches to identify deepfake audio. Mel-frequency cepstral coefﬁcients ({MFCCs}) technique is used to acquire the most useful information from the audio. We choose the Fake-or-Real dataset, which is the most recent benchmark dataset. The dataset was created with a text-to-speech model and is divided into four sub-datasets: for-rece, for-2-sec, fornorm and for-original. These datasets are classiﬁed into sub-datasets mentioned above according to audio length and bit rate. The experimental results show that the support vector machine ({SVM}) outperformed the other machine learning ({ML}) models in terms of accuracy on for-rece and for-2-sec datasets, while the gradient boosting model performed very well using for-norm dataset. The {VGG}-16 model produced highly encouraging results when applied to the for-original dataset. The {VGG}-16 model outperforms other state-of-the-art approaches.},
	pages = {134018--134028},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Hamza, Ameer and Javed, Abdul Rehman Rehman and Iqbal, Farkhund and Kryvinska, Natalia and Almadhor, Ahmad S. and Jalil, Zunera and Borghol, Rouba},
	urldate = {2024-05-28},
	date = {2022},
	langid = {english},
	file = {Hamza et al. - 2022 - Deepfake Audio Detection via MFCC Features Using M.pdf:/home/jacob/Zotero/storage/CEPJ2H3T/Hamza et al. - 2022 - Deepfake Audio Detection via MFCC Features Using M.pdf:application/pdf},
}

